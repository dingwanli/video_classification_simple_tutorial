{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dingwanli/.conda/envs/pytorch/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/raid/dingwanli/data/model_data_3Ds/dataset_non_bm_112f/train/'\n",
    "validate_path = '/raid/dingwanli/data/model_data_3Ds/dataset_non_bm_112f/validate/'\n",
    "\n",
    "MAX_frame = 23\n",
    "img_height = 250\n",
    "img_width = 250\n",
    "epoch = 50\n",
    "batch_size = 6\n",
    "\n",
    "save_model_path = \"E:/project_study/video-classification-master/Conv3D/output_model/\"  # save Pytorch models\n",
    "\n",
    "# 3D CNN parameters\n",
    "fc_hidden1, fc_hidden2 = 256, 256\n",
    "dropout = 0.0        # dropout probability\n",
    "\n",
    "# training parameters\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_3DCNN(data.Dataset):\n",
    "    \"Characterizes a dataset for PyTorch\"\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        \"Initialization\"\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the total number of samples\"\n",
    "        return len(os.listdir(self.data_path))\n",
    "\n",
    "    def read_images(self, sub_path, use_transform=None):\n",
    "        X = []\n",
    "        pic_name_required = []        \n",
    "        pic_name_list = os.listdir(sub_path)\n",
    "        \n",
    "        pic_name_list = sorted(pic_name_list, key=lambda x: str(x.split('.')[0].split('_')[-1]))\n",
    "        pic_name_list = sorted(pic_name_list, key=lambda x: len(x.split('.')[0].split('_')[-1]))\n",
    "        \n",
    "        num_frame = len(pic_name_list)\n",
    "        if num_frame > MAX_frame:\n",
    "            step_need = (num_frame)/(MAX_frame)\n",
    "            for step in range(MAX_frame):\n",
    "                required = int(step_need * step)\n",
    "                if int(step_need * (step+1)) >= num_frame:\n",
    "                    pic_name_required.append(pic_name_list[-1])\n",
    "                else:\n",
    "                    pic_name_required.append(pic_name_list[required])\n",
    "        else:\n",
    "            pic_name_required = pic_name_list.copy()\n",
    "        \n",
    "        for frame_name in pic_name_required:\n",
    "            frame_path = os.path.join(sub_path, frame_name)\n",
    "            image = Image.open(frame_path)\n",
    "            if use_transform is not None:\n",
    "                image = use_transform(image)\n",
    "#             image = torch.from_numpy(image)\n",
    "            X.append(image)\n",
    "        X = torch.stack(X, dim=0)\n",
    "        \n",
    "#         print(X.shape)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generates one sample of data\"\n",
    "        self.one_sequence_dir = os.listdir(self.data_path)\n",
    "        selected_sequence = self.one_sequence_dir[index]\n",
    "        one_sequence_path = os.path.join(self.data_path, selected_sequence) + '/'\n",
    "#         X = self.read_images(one_sequence_path, self.transform).unsqueeze_(0)\n",
    "        X = self.read_images(one_sequence_path, self.transform)\n",
    "        X = torch.transpose(X,0,1)\n",
    "        if os.listdir(one_sequence_path)[0].split(\"_\")[0] == \"A\":\n",
    "            y = torch.Tensor([1])\n",
    "        else:\n",
    "            y = torch.Tensor([0])\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize([img_height, img_width]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "\n",
    "use_cuda = torch.cuda.is_available()                   # check if GPU exists\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")   # use CPU or GPU\n",
    "\n",
    "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_set, valid_set = Dataset_3DCNN(train_path, transform=transform), \\\n",
    "                       Dataset_3DCNN(validate_path, transform=transform)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, **params)\n",
    "valid_loader = data.DataLoader(valid_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 23, 250, 250])\n",
      "torch.Size([6, 1])\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3D_output_size(img_size, padding, kernel_size, stride):\n",
    "    # compute output shape of conv3D\n",
    "    outshape = (np.floor((img_size[0] + 2 * padding[0] - (kernel_size[0] - 1) - 1) / stride[0] + 1).astype(int),\n",
    "                np.floor((img_size[1] + 2 * padding[1] - (kernel_size[1] - 1) - 1) / stride[1] + 1).astype(int),\n",
    "                np.floor((img_size[2] + 2 * padding[2] - (kernel_size[2] - 1) - 1) / stride[2] + 1).astype(int))\n",
    "    return outshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3D(nn.Module):\n",
    "    def __init__(self, t_dim=23, img_x=90, img_y=120, drop_p=0.2, fc_hidden1=256, fc_hidden2=128, num_classes=1):\n",
    "        super(CNN3D, self).__init__()\n",
    "\n",
    "        # set video dimension\n",
    "        self.t_dim = t_dim\n",
    "        self.img_x = img_x\n",
    "        self.img_y = img_y\n",
    "        # fully connected layer hidden nodes\n",
    "        self.fc_hidden1, self.fc_hidden2 = fc_hidden1, fc_hidden2\n",
    "        self.drop_p = drop_p\n",
    "        self.num_classes = num_classes\n",
    "        self.ch1, self.ch2 = 32, 48\n",
    "        self.k1, self.k2 = (5, 5, 5), (3, 3, 3)  # 3d kernel size\n",
    "        self.s1, self.s2 = (2, 2, 2), (2, 2, 2)  # 3d strides\n",
    "        self.pd1, self.pd2 = (0, 0, 0), (0, 0, 0)  # 3d padding\n",
    "\n",
    "        # compute conv1 & conv2 output shape\n",
    "        self.conv1_outshape = conv3D_output_size((self.t_dim, self.img_x, self.img_y), self.pd1, self.k1, self.s1)\n",
    "        self.conv2_outshape = conv3D_output_size(self.conv1_outshape, self.pd2, self.k2, self.s2)\n",
    "\n",
    "        self.conv1 = nn.Conv3d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1, stride=self.s1,\n",
    "                               padding=self.pd1)\n",
    "        self.bn1 = nn.BatchNorm3d(self.ch1)\n",
    "        self.conv2 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2, stride=self.s2,\n",
    "                               padding=self.pd2)\n",
    "        self.bn2 = nn.BatchNorm3d(self.ch2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.drop = nn.Dropout3d(self.drop_p)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "        self.fc1 = nn.Linear(self.ch2 * self.conv2_outshape[0] * self.conv2_outshape[1] * self.conv2_outshape[2],\n",
    "                             self.fc_hidden1)  # fully connected hidden layer\n",
    "        self.fc2 = nn.Linear(self.fc_hidden1, self.fc_hidden2)\n",
    "        self.fc3 = nn.Linear(self.fc_hidden2, self.num_classes)  # fully connected layer, output = multi-classes\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_3d):\n",
    "        # Conv 1\n",
    "        x = self.conv1(x_3d)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        # Conv 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop(x)\n",
    "        # FC 1 and 2\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "cnn3d = CNN3D(t_dim=MAX_frame, img_x=img_height, img_y=img_width,\n",
    "              drop_p=dropout, fc_hidden1=fc_hidden1,  fc_hidden2=fc_hidden2, num_classes=1).to(device)\n",
    "if torch.cuda.device_count() >= 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    cnn3d = nn.DataParallel(cnn3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 23, 250, 250)\n",
    "x = x.to(device)\n",
    "logits = cnn3d(x)  # (1,3)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  torch.Size([4, 3, 23, 250, 250])\n",
      "y:  tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "y.shape:  torch.Size([4, 1])\n",
      "\n",
      "output：  tensor([[0.0005],\n",
      "        [0.0003],\n",
      "        [0.0002],\n",
      "        [0.0001]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "output.shape:  torch.Size([4, 1])\n",
      "\n",
      "pred:  tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0', dtype=torch.uint8)\n",
      "pred_squeeze:  tensor([0, 0, 0, 0], device='cuda:0', dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "model = cnn3d\n",
    "for batch_idx, (X, y) in enumerate(train_loader):\n",
    "    print(\"X.shape: \", X.size())\n",
    "    print(\"y: \", y)\n",
    "    print(\"y.shape: \", y.shape)\n",
    "    print()\n",
    "    \n",
    "    X, y = X.to(device), y.to(device)\n",
    "    output = model(X)\n",
    "    print(\"output： \", output)\n",
    "    print(\"output.shape: \", output.shape)\n",
    "    print()\n",
    "    \n",
    "    pred = output.ge(0.5)\n",
    "    print(\"pred: \", pred)\n",
    "    \n",
    "    pred_squeeze = output.ge(0.5).squeeze()\n",
    "    print(\"pred_squeeze: \", pred_squeeze)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.cpu().data.squeeze())\n",
    "# print(output.cpu().data.squeeze())\n",
    "# y_pred = torch.max(output, 1)[1]\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    # set model as training mode\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    scores = []\n",
    "    N_count = 0   # counting total trained sample in one epoch\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "#         print(y)\n",
    "#         X, y = X.to(device), y.to(device).view(-1, )\n",
    "\n",
    "        N_count += X.size(0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)  # output size = (batch, number of classes)\n",
    "\n",
    "        loss = F.binary_cross_entropy(output, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # to compute accuracy\n",
    "#         y_pred = torch.max(output, 1)[1]  # y_pred != output\n",
    "#         step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        y_pred = output.ge(0.5)\n",
    "        step_score = accuracy_score(y.cpu().data.squeeze().numpy(), y_pred.cpu().data.squeeze().numpy())\n",
    "        \n",
    "        scores.append(step_score)         # computed on CPU\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # show information\n",
    "        if (batch_idx + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Accu: {:.2f}%'.format(\n",
    "                epoch + 1, N_count, len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item(), 100 * step_score))\n",
    "\n",
    "    return losses, scores\n",
    "\n",
    "\n",
    "def validation(model, device, optimizer, test_loader):\n",
    "    # set model as testing mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    all_y = []\n",
    "    all_y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            # distribute data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            output = model(X)\n",
    "\n",
    "            loss = F.binary_cross_entropy(output, y, reduction='sum')\n",
    "            test_loss += loss.item()                 # sum up batch loss\n",
    "#             y_pred = output.max(1, keepdim=True)[1]  # (y_pred != output) get the index of the max log-probability\n",
    "            y_pred = output.ge(0.5)\n",
    "\n",
    "            # collect all y and y_pred in all batches\n",
    "            all_y.extend(y)\n",
    "            all_y_pred.extend(y_pred)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # to compute accuracy\n",
    "    all_y = torch.stack(all_y, dim=0)\n",
    "    all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "    test_score = accuracy_score(all_y.cpu().data.squeeze().numpy(), all_y_pred.cpu().data.squeeze().numpy())\n",
    "\n",
    "    # show information\n",
    "    print('\\nTest set ({:d} samples): Average loss: {:.4f}, Accuracy: {:.2f}%\\n'.format(len(all_y), test_loss, 100* test_score))\n",
    "\n",
    "    # save Pytorch models of best record\n",
    "#     torch.save(model.state_dict(), os.path.join(save_model_path, '3dcnn_epoch{}.pth'.format(epoch + 1)))  # save spatial_encoder\n",
    "#     torch.save(optimizer.state_dict(), os.path.join(save_model_path, '3dcnn_optimizer_epoch{}.pth'.format(epoch + 1)))      # save optimizer\n",
    "#     print(\"Epoch {} model saved!\".format(epoch + 1))\n",
    "\n",
    "    return test_loss, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn3d.parameters(), lr=learning_rate)   # optimize all cnn parameters\n",
    "# record training process\n",
    "epoch_train_losses = []\n",
    "epoch_train_scores = []\n",
    "epoch_test_losses = []\n",
    "epoch_test_scores = []\n",
    "# start training\n",
    "for epoch in range(epochs):\n",
    "    # train, test model\n",
    "    train_losses, train_scores = train(log_interval, cnn3d, device, train_loader, optimizer, epoch)\n",
    "    epoch_test_loss, epoch_test_score = validation(cnn3d, device, optimizer, valid_loader)\n",
    "\n",
    "    # save results\n",
    "    epoch_train_losses.append(train_losses)\n",
    "    epoch_train_scores.append(train_scores)\n",
    "    epoch_test_losses.append(epoch_test_loss)\n",
    "    epoch_test_scores.append(epoch_test_score)\n",
    "\n",
    "    # save all train test results\n",
    "    A = np.array(epoch_train_losses)\n",
    "    B = np.array(epoch_train_scores)\n",
    "    C = np.array(epoch_test_losses)\n",
    "    D = np.array(epoch_test_scores)\n",
    "    np.save('E:/project_study/video-classification-master/Conv3D/outputs/3DCNN_epoch_training_losses.npy', A)\n",
    "    np.save('E:/project_study/video-classification-master/Conv3D/outputs/3DCNN_epoch_training_scores.npy', B)\n",
    "    np.save('E:/project_study/video-classification-master/Conv3D/outputs/3DCNN_epoch_test_loss.npy', C)\n",
    "    np.save('E:/project_study/video-classification-master/Conv3D/outputs/3DCNN_epoch_test_score.npy', D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1, epochs + 1), A[:, -1])  # train loss (on epoch end)\n",
    "plt.plot(np.arange(1, epochs + 1), C)         #  test loss (on epoch end)\n",
    "plt.title(\"model loss\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "# 2nd figure\n",
    "plt.subplot(122)\n",
    "plt.plot(np.arange(1, epochs + 1), B[:, -1])  # train accuracy (on epoch end)\n",
    "plt.plot(np.arange(1, epochs + 1), D)         #  test accuracy (on epoch end)\n",
    "# plt.plot(histories.losses_val)\n",
    "plt.title(\"training scores\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train', 'test'], loc=\"upper left\")\n",
    "title = \"./fig_UCF101_3DCNN.png\"\n",
    "plt.savefig(title, dpi=600)\n",
    "# plt.close(fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
